<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shraman Pramanick</title>
  
  <meta name="author" content="Shraman Pramanick">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="image/png" href="images/Blue_Jays.png">
  <title>Shraman Pramanick</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-113195086-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-113195086-1');
</script>

<style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #2d59eb;
      text-decoration: none;
    }
    b {
      color: #000000;
      text-decoration: none;
    }
    c {
      color: #0BDA51;
      text-decoration: none;
    }
    d {
      color: #FFA500;
      text-decoration: none;
    }
    a:focus,
    a:hover {
      color: #af19bf;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px
      background-image: "images/jose/bg.png";
       /*background-color: #fffff;*/
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 16px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 24px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 14.5px;
      font-weight: 600
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 34px;
    }
    
    .one {
      width: 160px;
      height: 160px;
      position: relative;
    }
    
    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }
    
    span.highlight {
      background-color: #CA7FD4;
    }
  </style>


</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shraman Pramanick</name>
              </p>
              <p style="text-align:justify">
              Hi! I am Shraman Pramanick. I am a 3<sup>rd</sup> year Ph.D. student at <a href="https://www.jhu.edu/">Johns Hopkins University</a>, in the Department of <a href="https://engineering.jhu.edu/ece/">Electrical and Computer Engineering</a> where I am working in the <a href="https://aiem.jhu.edu/">Artificial Intelligence for Engineering and Medicine (AIEM) Lab</a>, advised by <a href="https://engineering.jhu.edu/faculty/rama-chellappa/">Rama Chellappa</a>.
              </p>
              <p style="text-align:justify">
              My research focuses on applications of deep learning and computer vision to vision-language pre-training, multimodal learning (vision + X). In the summer of 2022, I was a research scientist intern at <a href="https://research.facebook.com/"> Meta AI</a>, where I worked with <a href="https://pzzhang.github.io/pzzhang/">Pengchuan Zhang</a> and <a href="http://people.csail.mit.edu/yalesong/home/">Yale Song</a> on egocentric video-language pre-training with its application to a wide range of egocentric downstreams including EgoMCQ, EgoNLQ, EgoMQ, QFVS, EgoTaskQA, CharadesEgo and Epic-Kitchens. I also collaborated with <a href="https://en.wikipedia.org/wiki/Yann_LeCun">Yann LeCun</a> and <a href="http://jingli.io/">Li Jing</a> on dimension-contrastive pre-training algorithms on image-text pairs.
              </p>  
              <p style="text-align:justify">
              Before joining Hopkins, I worked as a research assistant in a series of collaborative projects between <a href="https://www.qf.org.qa/research/qatar-computing-research-institute">QCRI, Doha</a> and <a href="https://www.iiitd.ac.in/">IIIT-Delhi</a>. During my undergrad days, I interned at the <a href="https://www.umontreal.ca/en/">University of Montreal</a> with <a href="https://www.mitacs.ca/en/programs/globalink/globalink-research-internship">Mitacs Globalink Fellowship</a> in the summer of 2019. I graduated from <a href="http://www.jaduniv.edu.in/">Jadavpur University, India</a>, in 2020 with my Bachelor's degree majoring in Electronics and Telecommunication Engineering.
              </p>
              <p style="text-align:center">
                <a href="mailto:spraman3@jhu.edu">Email</a> &nbsp/&nbsp
                <a href="data/Shraman_CV_July_2023.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=20SubC8AAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ShramanPramanick">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/shramanpramanick/">LinkedIn</a> 
              </p>
            </td>
            <td style="padding:0.75%;width:35%;max-width:35%">
              <a href="images/JonBarron.jpg"><img style="width:90%;max-width:90%" alt="profile photo" src="images/Shraman_Photo_2023_Cropped.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:-10px"><tbody>
            <tr style="padding:0px">
            <td style="padding:2.5%;vertical-align:middle;width:63%">
      <a id="news"><heading>News</heading></a>
      <p>
            <div style="width:100%;overflow-y:scroll; height:200px;">
                <ul id="News">
        <p>
        </p>
	<li> August, 2023 - 1 paper accepted in <b>TMLR 2023</b>.
	</li>
	<li> July, 2023 - 2 papers accepted in <b>ICCV 2023</b>.
	</li>
	<li> June, 2023 - Received student researcher offer from Google Research for Fall 2023.
	</li>
        <li> June, 2023 - Joined <b>Meta AI</b> as a returning research scientist intern. Working with <a href="https://www.linkedin.com/in/nicolas-ballas-a188583/">Nicolas Ballas</a>, <a href="https://www.linkedin.com/in/amjadalmahairi/">Amjad Almahairi</a>, <a href="https://guangxinghan.github.io/">Guangxing Han</a>, <a href="https://wqfcr.github.io/">Qifan Wang</a> and <a href="https://www.linkedin.com/in/rayhou/">Rui Hou</a>.
        </li>
        <li> February, 2023 - 1 paper accepted in <b>Biomedical Signal Processing and Control</b>, Elsevier.
        </li>
      <li> January, 2023 - Received research internship offers from Meta AI (return), Microsoft Research, Adobe Research, Salesforce Research, and Samsung Research America for Summer 2023.
        </li>
      <li> October, 2022 - Attended <b>ECCV 2022</b>, and presented our work <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2204.13861&sa=D&sntz=1&usg=AOvVaw1BYho5NwCrYK2z_9DG9hMj">Where in the World is this Image? Transformer-based Geo-localization in the Wild</a>. 
        </li>
        </li>
      <li> July, 2022 - 1 paper accepted in <b>ECCV 2022</b>. 
        </li>
      <li> May, 2022 - Joined <b>Meta AI</b> as a research scientist intern. Working with <a href="https://pzzhang.github.io/pzzhang/">Pengchuan Zhang</a>, <a href="http://jingli.io/">Li Jing</a>, <a href="http://people.csail.mit.edu/yalesong/home/">Yale Song</a>, and <a href="http://yann.lecun.com/">Yann LeCun</a>.
        </li>
      <li> January, 2022 - Attended <b>WACV 2022</b> in person at Waikoloa, Hawaii, and presented our work <a href="https://www.google.com/url?q=https%3A%2F%2Farxiv.org%2Fabs%2F2110.10949&sa=D&sntz=1&usg=AOvVaw1VF3ivOTXAAwz9ZuU8LMu7">Multimodal Learning using Optimal Transport for Sarcasm and Humor Detection</a>.
        </li>
      <li> November, 2021 - Received research internship offers from Meta AI, AWS AI, and Adobe Research for Summer 2022.
        </li>
      <li> October, 2021 - Received EMNLP volunteer fellowship and free registration in EMNLP 2021.
        </li>
      <li> October, 2021 - 1 paper accepted in <b>WACV 2022</b>.
        </li>
      <li> August, 2021 - 1 paper accepted in <b>Findings of EMNLP 2021</b>.
        </li>
      <li> July, 2021 - Received ACL volunteer fellowship and free registration in ACL 2021.
        </li>
      <li> May, 2021 - 1 paper accepted in <b>Knowledge-based Systems</b>, Elsevier.
        </li>
      <li> May, 2021 - 1 paper accepted in <b>Findings of ACL 2021</b>. 
        </li>
        <li> March, 2021 - 1 paper accepted in <b>ICWSM 2021</b>. 
        </li>
        <li> January, 2021 - Joined <b>Johns Hopkins University</b> for my Ph.D. with <b>ECE departmental fellowship</b>, under the supervision of Professor <a href="https://engineering.jhu.edu/faculty/rama-chellappa/">Rama Chellappa</a>.
        </li>
        <li> June, 2020 - Joined <b>Qatar Computing Research Institute</b> (QCRI), Doha as a Research Associate.
        </li>
        <li> April, 2020 - Received Ph.D. admission offers from 7 US Universities - JHU, GaTech, Purdue, UCR, Penn State, WashU St. Louis, and Yale. 
        </li>
        <li> January 2019 - I have been selected as one of the <a href="https://www.mitacs.ca/en/programs/globalink/globalink-research-internship">Mitacs Globalink Research Interns</a> 2019.
        </li>
        
            </ul>
            </div>
      </p>
    </td>
          </tr>
        </tbody></table>




        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a id="news"><heading>Research</heading></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/EgoVLPv2_System.gif" alt="EgoVLPv2" width="280" height="160">
            </div>
            </td>
            <td width="75%" valign="middle">
                 <a href=""> <papertitle>EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone</papertitle></a>           
            <br>
            <b>Shraman Pramanick</b>, <a href="http://people.csail.mit.edu/yalesong/home/">Yale Song</a>, <a href="https://sayannag.github.io/">Sayan Nag</a>, <a href="https://qinghonglin.github.io/">Kevin Qinghong Lin</a>, <a href="https://www.linkedin.com/in/hardik-shah-75ab5429/">Hardik Shah</a>, <a href="https://sites.google.com/view/showlab">Mike Z. Shou</a>, <a href="https://engineering.jhu.edu/ece/faculty/rama-chellappa/">Rama Chellappa</a>, <a href="https://qinghonglin.github.io/">Pengchuan Zhang</a>    
            <br>
            <em>ICCV, 2023</em>

        <p> <a href="https://arxiv.org/pdf/2307.05463.pdf">Paper</a> | <a href="https://github.com/facebookresearch/EgoVLPv2">Code</a> | <a href="https://shramanpramanick.github.io/EgoVLPv2/">Project</a>
            </td>
          </tr>


          <tr>
            <td style="padding:5px;width:40%;vertical-align:middle">
              <div style="text-align: center">
              <img src="images/UniVTG.jpg" alt="UniVTG" width="180" height="110" class="center">
              </div>
            </td>
            <td width="75%" valign="middle">
                 <a href=""> <papertitle>UniVTG: Towards Unified Video-Language Temporal Grounding</papertitle></a>           
            <br>
            <a href="https://qinghonglin.github.io/">Kevin Qinghong Lin</a>, <a href="https://qinghonglin.github.io/">Pengchuan Zhang</a>, Joya Chen, <b>Shraman Pramanick</b>, Difei Gao, Alex JP. Wang, Rui Yan, <a href="https://sites.google.com/view/showlab">Mike Z. Shou</a>   
            <br>
            <em>ICCV, 2023</em>

        <p> <a href="https://arxiv.org/pdf/2307.16715.pdf">Paper</a> | <a href="https://github.com/showlab/UniVTG">Code</a>
            </td>
          </tr>


          <tr>
            <td style="padding:15px;width:40%;vertical-align:middle;padding-right:20px">
              <div style="text-align: center">
              <img src="images/VoLTA_Alignment.png" alt="VoLTA" width="210" height="160">
            </div>
            </td>
            <td width="75%" valign="middle">
                 <a href="https://arxiv.org/pdf/2210.04135.pdf"> <papertitle>VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment</papertitle></a>           
            <br>
            <b>Shraman Pramanick*</b>, <a href="http://jingli.io/">Li Jing*</a>, <a href="https://sayannag.github.io/">Sayan Nag*</a>, <a href="https://jiachenzhu.github.io/">Jiachen Zhu</a>, <a href="https://www.linkedin.com/in/hardik-shah-75ab5429/">Hardik Shah</a>, <a href="https://en.wikipedia.org/wiki/Yann_LeCun">Yann LeCun</a>, <a href="https://engineering.jhu.edu/faculty/rama-chellappa/">Rama Chellappa</a>
            <br>
            <em>TMLR, 2023</em>

        <p> <a href="https://arxiv.org/pdf/2210.04135.pdf">Paper</a> | Code (Coming Soon) | Project (Coming Soon) </p>
            </td>
          </tr>

          <tr>
            <td style="padding:15px;width:40%;vertical-align:middle">
              <div style="text-align: center;">
              <img src="images/TransLocator_System.png" alt="TransLocator" width="160" height="160" class="center">
            </div>
            </td>
            <td width="75%" valign="middle">
                 <a href="https://arxiv.org/pdf/2204.13861.pdf"> <papertitle>Where in the World is this Image? Transformer-based Geo-localization in the Wild</papertitle></a>           
            <br>
            <b>Shraman Pramanick</b>, <a href="https://ewanowara.github.io/">Ewa Nowara</a>, Joshua Gleason, <a href="https://www.linkedin.com/in/carlos-castillo-7800113a/">Carlos D. Castillo</a>, <a href="https://engineering.jhu.edu/faculty/rama-chellappa/">Rama Chellappa</a>
            <br>
            <em>ECCV, 2022</em>

        <p> <a href="https://arxiv.org/pdf/2204.13861.pdf">Paper</a> | <a href="https://github.com/ShramanPramanick/Transformer_Based_Geo-localization">Code + Data</a> | <a href="https://www.cis.jhu.edu/~shraman/TransLocator/ECCV_2022_TransLocator_Slides.pdf">Slides</a> | <a href="https://www.cis.jhu.edu/~shraman/TransLocator/TransLocator_Poster.pdf">Poster</a> | <a href="https://www.cis.jhu.edu/~shraman/TransLocator/TransLocator_Presentation.mp4">Video</a> </p>
            </td>
          </tr>

          <tr>
            <td style="padding:15px;width:40%;vertical-align:middle">
              <div style="text-align: center;">
              <img src="images/MuLOT_System.png" alt="MuLOT" width="220" height="160" class="center">
            </div>
            </td>
            <td width="75%" valign="middle">
                 <a href="https://arxiv.org/pdf/2110.10949.pdf"> <papertitle>Multimodal Learning using Optimal Transport for Sarcasm and Humor Detection</papertitle></a>           
            <br>
            <b>Shraman Pramanick*</b>, <a href="https://sites.google.com/site/aniketsealiitkgp/home">Aniket Roy*</a>, <a href="https://engineering.jhu.edu/vpatel36/team/vishalpatel/">Vishal M. Patel</a>
            <br>
            <em>WACV, 2022</em>

        <p> <a href="https://arxiv.org/pdf/2110.10949.pdf">Paper</a> | <a href="https://www.cis.jhu.edu/~shraman/MuLOT/WACV_2022_MuLOT_slides.pdf">Slides</a> | <a href="https://www.cis.jhu.edu/~shraman/MuLOT/MuLOT_Poster.pdf">Poster</a> | <a href="https://www.cis.jhu.edu/~shraman/MuLOT/MuLOT_Presentation.mp4">Video</a> </p>
            </td>
          </tr>

          <tr>
            <td style="padding:15px;width:40%;vertical-align:middle">
              <div style="text-align: center;">
              <img src="images/MOMENTA_System.png" alt="MuLOT" width="230" height="90" class="center">
            </div>
            </td>
            <td width="75%" valign="middle">
                 <a href="https://arxiv.org/pdf/2109.05184.pdf"> <papertitle>MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets</papertitle></a>           
            <br>
            <b>Shraman Pramanick*</b>, Shivam Sharma*, Dimitar Dimitrov, <a href="https://www.iiitd.ac.in/shad">Shad Aktar</a>, <a href="https://mbzuai.ac.ae/study/faculty/preslav-nakov">Preslav Nakov</a>, <a href="https://tanmoychak.com/">Tanmoy Chakraborty</a> 
            <br>
            <em>Findings of EMNLP, 2021</em>

        <p> <a href="https://arxiv.org/pdf/2109.05184.pdf">Paper</a> | <a href="https://github.com/lcs2-iiitd/momenta">Code + Data</a> | <a href="https://www.cis.jhu.edu/~shraman/MOMENTA_poster.pdf">Poster</a> | <a href="https://www.cis.jhu.edu/~shraman/MOMENTA_slides.pdf">Slides</a> </p>
            </td>
          </tr>


          <tr>
            <td style="padding:15px;width:40%;vertical-align:middle">
              <div style="text-align: center;">
              <img src="images/BCI_System_Overview.png" alt="BCI" width="280" height="160" class="center">
            </div>
            </td>
            <td width="75%" valign="middle">
                 <a href="https://www.sciencedirect.com/science/article/pii/S1746809423001982"> <papertitle>Localizing and Grasping of 3-D Objects by a Vision-Actuated Robot Arm using Brain-Computer Interface</papertitle></a>           
            <br>
            Arnab Rakshit, <b>Shraman Pramanick</b>, Anurag Bagchi, Saugat Bhattacharyya 
            <br>
            <em>Biomedical Signal Processing and Control, Elsevier, 2023</em>

        <p> <a href="https://www.sciencedirect.com/science/article/pii/S1746809423001982">Paper</a> </p>
            </td>
          </tr>

          <tr>
            <td style="padding:15px;width:40%;vertical-align:middle">
              <div style="text-align: center;">
              <img src="images/ACL_Harmful_Memes_Explainability.png" alt="ACL_Harmful_Memes" width="210" height="160" class="center">
            </div>
            </td>
            <td width="75%" valign="middle">
                 <a href="https://aclanthology.org/2021.findings-acl.246.pdf"> <papertitle>Detecting Harmful Memes and Their Targets </papertitle></a>           
            <br>
            <b>Shraman Pramanick</b>, Dimitar Dimitrov, Rituparna Mukherjee, Shivam Sharma, <a href="https://www.iiitd.ac.in/shad">Shad Aktar</a>, <a href="https://mbzuai.ac.ae/study/faculty/preslav-nakov">Preslav Nakov</a>, <a href="https://tanmoychak.com/">Tanmoy Chakraborty</a> 
            <br>
            <em>Findings of ACL, 2021</em>

        <p> <a href="https://aclanthology.org/2021.findings-acl.246.pdf">Paper</a> | <a href="https://github.com/di-dimitrov/harmeme">Code + Data</a> | <a href="https://www.cis.jhu.edu/~shraman/ACL_2021_Harmeme_Presentation.pdf">Slides</a> | <a href="https://www.youtube.com/watch?v=E7TswYcUqd8&t=58s">Video</a> </p>
            </td>
          </tr>

          <tr>
            <td style="padding:15px;width:40%;vertical-align:middle">
              <div style="text-align: center;">
              <img src="images/FLORAL_System.png" alt="FLORAL" width="230" height="90" class="center">
            </div>
            </td>
            <td width="75%" valign="middle">
                 <a href="https://www.sciencedirect.com/science/article/abs/pii/S0950705121004159?casa_token=iJLWbW5cPwAAAAAA:oILGjoOQlOhRlKd2YHoYnqVPSDLSUSc-dwyUk0myxoEBn3T7-adXs4R1RB49KmWtB2d3lMxud0M"> <papertitle>See, Hear, Read: Leveraging Multimodality with Guided Attention for Abstractive Text Summarization</papertitle></a>           
            <br>
            Yash Atri*, <b>Shraman Pramanick*</b>, Vikram Goyal, <a href="https://tanmoychak.com/">Tanmoy Chakraborty</a> 
            <br>
            <em>Knowledge-Based Systems, Elsevier, 2021</em>

        <p> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0950705121004159?casa_token=iJLWbW5cPwAAAAAA:oILGjoOQlOhRlKd2YHoYnqVPSDLSUSc-dwyUk0myxoEBn3T7-adXs4R1RB49KmWtB2d3lMxud0M">Paper</a> | <a href="https://www.google.com/url?q=https%3A%2F%2Fgithub.com%2FLCS2-IIITD%2Fmultimodal_summ&sa=D&sntz=1&usg=AOvVaw1wtGbFeAnzjsDSliXBjCED">Code + Data</a> </p>
            </td>
          </tr>


          <tr>
            <td style="padding:15px;width:40%;vertical-align:middle">
              <div style="text-align: center;">
              <img src="images/ICWSM_System.PNG" alt="ICWSM_MHA_Meme_Model" width="230" height="90" class="center">
            </div>
            </td>
            <td width="75%" valign="middle">
                 <a href="https://ojs.aaai.org/index.php/ICWSM/article/view/18080/17883"> <papertitle>Exercise? I thought you said ’Extra Fries’: Leveraging Sentence Demarcations and Multi-hop Attention for Meme Affect Analysis</papertitle></a>           
            <br>
            <b>Shraman Pramanick</b>, <a href="https://www.iiitd.ac.in/shad">Shad Aktar</a>,<a href="https://tanmoychak.com/">Tanmoy Chakraborty</a> 
            <br>
            <em>ICWSM, 2021</em>

        <p> <a href="https://ojs.aaai.org/index.php/ICWSM/article/view/18080/17883">Paper</a> | <a href="https://github.com/LCS2-IIITD/MHA-MEME">Code</a> | <a href="https://www.cis.jhu.edu/~shraman/icwsm_presentation.pdf">Slides</a> | <a href="https://www.cis.jhu.edu/~shraman/ICWSM_Research_Highlights.pdf">Poster</a> </p>
            </td>
          </tr>


        </tbody></table>

        
        <br>
        <table width="100%" align="center" border="0" cellpadding="0">
          <tr style="padding:0px">
            <td width="100%" valign="middle">
              <a id="news"><heading>Teaching</heading></a>
            </td>
          </tr>
          <tr>
            <td width="75%" valign="center">
              <p>
              <p>I have been worked as TA for the following courses:</p>
              <ul>
                <li>
                  <papertitle>Spring 2023</papertitle>: Machine Intelligence (EN.520.650), Johns Hopkins University
                </li>
                <li>
                  <papertitle>Spring 2022</papertitle>: Machine Intelligence (EN.520.650), Johns Hopkins University
                </li>
              </ul>
              </p>
            </td>
          </tr>
        </table>
          

        <br>
        <table width="100%" align="center" border="0" cellpadding="0">
          <tr style="padding:0px">
            <td width="100%" valign="middle">
              <a id="news"><heading>Selected Honors & Awards</heading></a>
            </td>
          </tr>
          <tr>
            <td width="75%" valign="center">
              <p>
              <ul>
                <li>
                January, 2021 - <a href="https://engineering.jhu.edu/education/graduate-studies/full-time-graduate-fellowships-grants/">JHU ECE Departmental Fellowship</a>, <em>awarded to outstanding incoming Ph.D. students</em>.
                </li>
                <li>
                May, 2019 - <a href="https://www.mitacs.ca/en/programs/globalink/globalink-research-internship">Mitacs Globalink Research Internship</a>, awarded to top-ranked applicants from 15 different countries to participate in a 12-week research internship in Canadian universities.
                </li>
                <li>
                October, 2016 - <a href="https://jbnsts.ac.in/seshprg.php">JBNSTS Senior Scholarship</a>, 4-year scholarship for academic excellence during B.E.
                </li>
                <li>
                January, 2015 - <a href="https://olympiads.hbcse.tifr.res.in/about-olympiads/stages/mathematical-olympiad/">Regional Mathematical Olympiad (RMO)</a>, ranked among top-10 students in the state.
                </li>
              </ul>
              </p>
            </td>
          </tr>
        </table>

        
        <br>
        <table width="100%" align="center" border="0" cellpadding="0">
          <tr style="padding:0px">
            <td width="100%" valign="middle">
              <a id="news"><heading>Voluntary Services</heading></a>
            </td>
          </tr>
          <tr>
            <td width="75%" valign="center">
              <p>
              <p>I have reviewed for:</p>
              <ul>
                <li>
                  <papertitle>Conferences</papertitle>: CVPR, ICCV, ECCV, WACV, EMNLP, ACL
                </li>
                <li>
                  <papertitle>Jounrals</papertitle>: TPAMI, TNNLS, TAI, TAFFC
                </li>
              </ul>
              </p>
            </td>
          </tr>
        </table>


				

      
					
				
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <br>
                Stolen from <a href="https://jonbarron.info/">Jon Barron</a>. Big thanks! 
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
